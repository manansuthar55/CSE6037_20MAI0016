{"cells":[{"metadata":{},"cell_type":"markdown","source":"# VIT-Vellore, SCOPE\n## CSE6037 - Deep Learning and its Applications\n### SUTHAR MANAN BHARATKUMAR 20MAI0016\n### Assessment 2\n***GitHub Link:*** https://github.com/manansuthar55/CSE6037_20MAI0016/tree/main/Assessment_2\n#### Problem 1: Demonstrate Activation Functions used in Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"# Activation Functions for Neural Networks\n\nActivation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.\n\n![](https://missinglink.ai/wp-content/uploads/2018/11/activationfunction-1.png)\n\n\nThe Activation Functions can be basically divided into 2 types-\n1. Linear Activation Function\n2. Non-linear Activation Functions"},{"metadata":{},"cell_type":"markdown","source":"### There are several activations which are generally preffered, which we will see here... \n\n## 1. Step function\nThe first thing that comes to our mind when we have an activation function would be a threshold based classifier i.e. whether or not the neuron should be activated based on the value from the linear transformation. In other words, if the input to the activation function is greater than a threshold, then the neuron is activated, else it is deactivated, i.e. its output is not considered for the next hidden layer.\n\n\n![](https://miro.medium.com/max/1646/1*pmFsxtfiFoo09rXLaKyrOg.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ndemonp = np.random.uniform(-5, 5, (5))\ndemonp","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"array([ 4.71378107, -2.47828926,  3.85750137, -4.81662257, -2.92057655])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_step(x):\n    if x<0:\n        return 0\n    else:\n        return 1\nstep=[]\nfor i in demonp:\n    step.append(binary_step(i))\nstep","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"[1, 0, 1, 0, 0]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2. Sigmoid\nThe main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points. The function is monotonic but function’s derivative is not. The logistic sigmoid function can cause a neural network to get stuck at the training time.\n\n\n![](https://miro.medium.com/max/485/1*Xu7B5y9gp0iL5ooBj7LtWw.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nsgmd = sigmoid(demonp)\nsgmd","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"array([0.99110897, 0.07739427, 0.97931615, 0.00802909, 0.05114571])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 3. Tanh or hyperbolic tangent\n\ntanh is also like logistic sigmoid but better. The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped). The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. The function is differentiable. The function is monotonic while its derivative is not monotonic. The tanh function is mainly used classification between two classes. Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n![](https://miro.medium.com/max/595/1*f9erByySVjTjohfFdNkJYQ.jpeg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef tanhactfun(x):\n    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\ntan = tanhactfun(demonp)\ntan","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"array([ 0.99983906, -0.98602442,  0.99910823, -0.99986898, -0.99420585])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 4. ReLU (Rectified Linear Unit)\nThe ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning. As you can see, the ReLU is half rectified (from bottom). f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero.\n\nRange: 0 to infinity\n\nThe function and its derivative both are monotonic. But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.\n![](https://miro.medium.com/max/700/1*XxxiA0jJvPrHEJHD4z893g.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reluactfun(x):\n    return (max(0,x))\nreluact=[]\nfor i in demonp:\n    reluact.append(reluactfun(i))\nreluact","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[4.713781074034406, 0, 3.8575013747604565, 0, 0]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 5. Leaky ReLU\nIt is an attempt to solve the dying ReLU problem. The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so. When a is not 0.01 then it is called Randomized ReLU. Therefore the range of the Leaky ReLU is (-infinity to infinity). Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.\n![](https://miro.medium.com/max/2050/1*siH_yCvYJ9rqWSUYeDBiRA.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reluactfun(x):\n    return (max(0.1*x, x))\nreluact=[]\nfor i in demonp:\n    reluact.append(reluactfun(i))\nreluact","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"[4.713781074034406,\n -0.24782892640304022,\n 3.8575013747604565,\n -0.4816622574567537,\n -0.2920576546944821]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 6. ELU (Exponential Linear Unit)\n\nExponential Linear Unit or ELU for short is also a variant of Rectiufied Linear Unit (ReLU) that modifies the slope of the negative part of the function. Unlike the leaky relu and parametric ReLU functions, instead of a straight line, ELU uses a log curve for defning the negatice values.\n\n\n![](https://360digit.b-cdn.net/assets/admin/ckfinder/userfiles/images/blog/elu.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def elu_function(x, a):\n    return max(a*(np.exp(x)-1),x)\nelu=[]\na = 0.55\nfor i in demonp:\n    elu.append(elu_function(i,a))\nelu","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"[60.760068146359714,\n -0.503862364086107,\n 25.49079570769976,\n -0.5455482570054477,\n -0.5203535694720284]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 7. Swish\n\nSwish is a lesser known activation function which was discovered by researchers at Google. Swish is as computationally efficient as ReLU and shows better performance than ReLU on deeper models.  The values for swish ranges from negative infinity to infinity.\n\n![](https://miro.medium.com/max/1248/1*rBfSD7_bNCmmhGnyksNEPg.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def swish_function(x):\n    return x/(1-np.exp(-x))\nswish=[]\nfor i in demonp:\n    swish.append(swish_function(i))\nswish","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"[4.756450209698867,\n 0.22693182881506965,\n 3.9407324272723794,\n 0.03930425132587174,\n 0.166395840790475]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 8. Softmax\nSoftmax function is often described as a combination of multiple sigmoids. We know that sigmoid returns values between 0 and 1, which can be treated as probabilities of a data point belonging to a particular class. Thus sigmoid is widely used for binary classification problems. The softmax function can be used for multiclass classification problems. \n\n![](https://www.researchgate.net/profile/Binghui_Chen/publication/319121953/figure/fig2/AS:527474636398592@1502771161390/Softmax-activation-function.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax_function(x):\n    z = np.exp(x)\n    z_ = z/z.sum()\n    return z_\nsoftm = softmax_function(demonp)\nsoftm","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array([7.01238671e-01, 5.27702798e-04, 2.97843626e-01, 5.09171566e-05,\n       3.39083354e-04])"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}